<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Est-ce que ChatGPT sait ce qu'est une question? | Christian Jauvin</title><meta name=keywords content><meta name=description content="J&rsquo;expliquais récemment à un ami que ChatGPT, dans son essence, est « juste » un
modèle de prédiction du mot suivant, celui qui vient après une suite d&rsquo;autres
mots. Ainsi, quand on lui demande « Quelle est la capitale de la France ? », il
ne répond pas (vraiment) à la question : il complète plutôt une séquence de mots
sur laquelle il a été entraîné, en profondeur et avec une très grande
efficacité."><meta name=author content="Christian Jauvin"><link rel=canonical href=https://cjauvin.github.io/posts/llm-question-reponse/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://cjauvin.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://cjauvin.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://cjauvin.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://cjauvin.github.io/apple-touch-icon.png><link rel=mask-icon href=https://cjauvin.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://cjauvin.github.io/posts/llm-question-reponse/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><style>.first-entry.home-info{min-height:auto!important;margin:calc(var(--gap)/2)0 var(--gap)!important}.first-entry.home-info .entry-content{margin:8px 0!important}.first-entry.home-info .entry-footer{margin-top:8px!important}.social-icons a{padding:5px!important}.main{padding:calc(var(--gap)/2)var(--gap)!important}.center{text-align:center}.center img{display:block;margin-left:auto;margin-right:auto}</style><meta property="og:url" content="https://cjauvin.github.io/posts/llm-question-reponse/"><meta property="og:site_name" content="Christian Jauvin"><meta property="og:title" content="Est-ce que ChatGPT sait ce qu'est une question?"><meta property="og:description" content="J’expliquais récemment à un ami que ChatGPT, dans son essence, est « juste » un modèle de prédiction du mot suivant, celui qui vient après une suite d’autres mots. Ainsi, quand on lui demande « Quelle est la capitale de la France ? », il ne répond pas (vraiment) à la question : il complète plutôt une séquence de mots sur laquelle il a été entraîné, en profondeur et avec une très grande efficacité."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-26T11:21:12-05:00"><meta property="article:modified_time" content="2026-02-26T11:21:12-05:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Est-ce que ChatGPT sait ce qu'est une question?"><meta name=twitter:description content="J&rsquo;expliquais récemment à un ami que ChatGPT, dans son essence, est « juste » un
modèle de prédiction du mot suivant, celui qui vient après une suite d&rsquo;autres
mots. Ainsi, quand on lui demande « Quelle est la capitale de la France ? », il
ne répond pas (vraiment) à la question : il complète plutôt une séquence de mots
sur laquelle il a été entraîné, en profondeur et avec une très grande
efficacité."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://cjauvin.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Est-ce que ChatGPT sait ce qu'est une question?","item":"https://cjauvin.github.io/posts/llm-question-reponse/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Est-ce que ChatGPT sait ce qu'est une question?","name":"Est-ce que ChatGPT sait ce qu\u0027est une question?","description":"J\u0026rsquo;expliquais récemment à un ami que ChatGPT, dans son essence, est « juste » un modèle de prédiction du mot suivant, celui qui vient après une suite d\u0026rsquo;autres mots. Ainsi, quand on lui demande « Quelle est la capitale de la France ? », il ne répond pas (vraiment) à la question : il complète plutôt une séquence de mots sur laquelle il a été entraîné, en profondeur et avec une très grande efficacité.\n","keywords":[],"articleBody":"J’expliquais récemment à un ami que ChatGPT, dans son essence, est « juste » un modèle de prédiction du mot suivant, celui qui vient après une suite d’autres mots. Ainsi, quand on lui demande « Quelle est la capitale de la France ? », il ne répond pas (vraiment) à la question : il complète plutôt une séquence de mots sur laquelle il a été entraîné, en profondeur et avec une très grande efficacité.\nEn considérant cela, on pourrait dire que ChatGPT se trouve dans une situation comparable à la vôtre si quelqu’un vous récitait une suite de mots que vous ne comprenez pas (dans une langue étrangère, disons), puis qu’une autre personne vous tendait un carton sur lequel figurent des mots à prononcer en guise de réponse (dans une langue que vous ne comprenez pas mais que vous pouvez lire, disons).\nEn un sens donc, vous (et ChatGPT) vous trouvez dans une situation analogue à celle de l’opérateur dans la Chambre chinoise de Searle : vous pouvez manipuler efficacement et de manière procédurale un ensemble de symboles, mais ceux-ci n’ont aucun sens pour vous. Vous êtes aveugle à leur signification réelle.\nMais alors, qu’est-ce que c’est, au fond, que l’acte de répondre à une question ? Pourquoi la manière dont ChatGPT répond nous semblerait-elle « moins » authentique que la nôtre, exactement ? Que signifie même le fait de comprendre que quelque chose est une question ? Wittgenstein dirait probablement que toutes les situations impliquant le fait de répondre font partie d’un vaste jeu de langagehttps://fr.wikipedia.org/wiki/Jeu_de_langage_(philosophie), soit un ensemble de comportements humains que l’on regroupe, de manière générale, sous l’appellation « répondre à une question ». Il n’y a pas d’essence profonde et unique de la « réponse-ité », ni de la « question-ité » : il y a une multitude de comportements vaguement apparentés, impliquant des contextes particuliers et le langage, que nous désignons ainsi.\nCe que cela pose comme problème, selon moi, en est un de modélisation a priori : depuis aussi longtemps que l’idée d’IA existe, le modèle a priori de la compréhension du langage (et par implication, de la capacité à répondre à des questions) a toujours été le suivant : on reçoit une séquence de symboles (des mots), puis un mécanisme de traitement se charge de « comprendre » ces mots, c’est-à-dire de construire une sorte de représentation (interne) de leur signification, quelle que soit la nature de cette « chose » (un modèle mental interne, un état, une configuration, peu importe). Ce n’est qu’à partir de là, une fois cette représentation acquise, que quelque chose peut s’ensuivre (une réponse, dans le cas où l’on répond à une question).\nOr il semble que la modélisation avec les grands modèles de langage remette ce schéma en question. Il semble qu’une compréhension réelle puisse émerger du simple fait d’apprendre à prédire adéquatement une séquence de mots, sans qu’il soit nécessaire de passer par cette modélisation intermédiaire, cette boîte noire qui « comprend » quelque chose. Et c’est là un changement de paradigme très profond et déstabilisant.\n","wordCount":"510","inLanguage":"en","datePublished":"2026-02-26T11:21:12-05:00","dateModified":"2026-02-26T11:21:12-05:00","author":{"@type":"Person","name":"Christian Jauvin"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://cjauvin.github.io/posts/llm-question-reponse/"},"publisher":{"@type":"Organization","name":"Christian Jauvin","logo":{"@type":"ImageObject","url":"https://cjauvin.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://cjauvin.github.io/ accesskey=h title="Christian Jauvin (Alt + H)">Christian Jauvin</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Est-ce que ChatGPT sait ce qu'est une question?</h1><div class=post-meta><span title='2026-02-26 11:21:12 -0500 -0500'>February 26, 2026</span>&nbsp;·&nbsp;Christian Jauvin</div></header><div class=post-content><p>J&rsquo;expliquais récemment à un ami que ChatGPT, dans son essence, est « juste » un
modèle de prédiction du mot suivant, celui qui vient après une suite d&rsquo;autres
mots. Ainsi, quand on lui demande « Quelle est la capitale de la France ? », il
ne répond pas (vraiment) à la question : il complète plutôt une séquence de mots
sur laquelle il a été entraîné, en profondeur et avec une très grande
efficacité.</p><p>En considérant cela, on pourrait dire que ChatGPT se trouve dans une situation
comparable à la vôtre si quelqu&rsquo;un vous récitait une suite de mots que vous ne
comprenez pas (dans une langue étrangère, disons), puis qu&rsquo;une autre personne
vous tendait un carton sur lequel figurent des mots à prononcer en guise de
réponse (dans une langue que vous ne comprenez pas mais que vous pouvez lire,
disons).</p><p class=center><img loading=lazy src=/posts/llm-question-reponse/llm-question-searle.png></p><p>En un sens donc, vous (et ChatGPT) vous trouvez dans une situation analogue à
celle de l&rsquo;opérateur dans la <a href=https://fr.wikipedia.org/wiki/Chambre_chinoise>Chambre chinoise de
Searle</a> : vous pouvez manipuler
efficacement et de manière procédurale un ensemble de symboles, mais ceux-ci
n&rsquo;ont aucun sens pour vous. Vous êtes aveugle à leur signification réelle.</p><p>Mais alors, qu&rsquo;est-ce que c&rsquo;est, au fond, que l&rsquo;acte de répondre à une question
? Pourquoi la manière dont ChatGPT répond nous semblerait-elle « moins »
authentique que la nôtre, exactement ? Que signifie même le fait de comprendre
que quelque chose est une question ? Wittgenstein dirait probablement que toutes
les situations impliquant le fait de répondre font partie d&rsquo;un vaste <a href>jeu de
langage</a><a href=https://fr.wikipedia.org/wiki/Jeu_de_langage_(philosophie)>https://fr.wikipedia.org/wiki/Jeu_de_langage_(philosophie)</a>, soit un
ensemble de comportements humains que l&rsquo;on regroupe, de manière générale, sous
l&rsquo;appellation « répondre à une question ». Il n&rsquo;y a pas d&rsquo;essence profonde et
unique de la « réponse-ité », ni de la « question-ité » : il y a une multitude
de comportements vaguement apparentés, impliquant des contextes particuliers et
le langage, que nous désignons ainsi.</p><p>Ce que cela pose comme problème, selon moi, en est un de modélisation a priori :
depuis aussi longtemps que l&rsquo;idée d&rsquo;IA existe, le modèle a priori de la
compréhension du langage (et par implication, de la capacité à répondre à des
questions) a toujours été le suivant : on reçoit une séquence de symboles (des
mots), puis un mécanisme de traitement se charge de « comprendre » ces mots,
c&rsquo;est-à-dire de construire une sorte de représentation (interne) de leur
signification, quelle que soit la nature de cette « chose » (un modèle mental
interne, un état, une configuration, peu importe). Ce n&rsquo;est qu&rsquo;à partir de là,
une fois cette représentation acquise, que quelque chose peut s&rsquo;ensuivre (une
réponse, dans le cas où l&rsquo;on répond à une question).</p><p class=center><img loading=lazy src=/posts/llm-question-reponse/llm-question-answer-classic-ai.png></p><p>Or il semble que la modélisation avec les grands modèles de langage remette ce
schéma en question. Il semble qu&rsquo;une compréhension réelle puisse émerger du
simple fait d&rsquo;apprendre à prédire adéquatement une séquence de mots, sans qu&rsquo;il
soit nécessaire de passer par cette modélisation intermédiaire, cette boîte
noire qui « comprend » quelque chose. Et c&rsquo;est là un changement de paradigme
très profond et déstabilisant.</p><p class=center><img loading=lazy src=/posts/llm-question-reponse/llm-question-answer-wittgenstein.png></p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://cjauvin.github.io/>Christian Jauvin</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>