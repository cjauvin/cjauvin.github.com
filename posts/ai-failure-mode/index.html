<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>We didn't get the AI failure modes that philosophy anticipated | Christian Jauvin</title><meta name=keywords content><meta name=description content="The original idea of AI, that we got mostly through science-fiction, and also a
little from the philosophy of mind and logic, imagined an entity that would
implement idealized and mechanical notions of thoughts, reasoning and logic.
Such an entity would of course know everything there is to know about such
topics, and its behavior would thus be rooted in them. Although this would mean
that the entity would generally behave in impressive and powerful ways, it was
also implicitly understood that sometimes this &ldquo;perfection&rdquo; would lead to
paradoxical behaviors and &ldquo;errors&rdquo;: the robot stuck in a circle in the Asimov
story is the quintessential example."><meta name=author content><link rel=canonical href=https://cjauvin.github.io/posts/ai-failure-mode/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://cjauvin.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://cjauvin.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://cjauvin.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://cjauvin.github.io/apple-touch-icon.png><link rel=mask-icon href=https://cjauvin.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://cjauvin.github.io/posts/ai-failure-mode/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><style>.first-entry.home-info{min-height:auto!important;margin:calc(var(--gap)/2)0 var(--gap)!important}.first-entry.home-info .entry-content{margin:8px 0!important}.first-entry.home-info .entry-footer{margin-top:8px!important}.social-icons a{padding:5px!important}.main{padding:calc(var(--gap)/2)var(--gap)!important}.center{text-align:center}.center img{display:block;margin-left:auto;margin-right:auto}</style><meta property="og:url" content="https://cjauvin.github.io/posts/ai-failure-mode/"><meta property="og:site_name" content="Christian Jauvin"><meta property="og:title" content="We didn't get the AI failure modes that philosophy anticipated"><meta property="og:description" content="The original idea of AI, that we got mostly through science-fiction, and also a little from the philosophy of mind and logic, imagined an entity that would implement idealized and mechanical notions of thoughts, reasoning and logic. Such an entity would of course know everything there is to know about such topics, and its behavior would thus be rooted in them. Although this would mean that the entity would generally behave in impressive and powerful ways, it was also implicitly understood that sometimes this “perfection” would lead to paradoxical behaviors and “errors”: the robot stuck in a circle in the Asimov story is the quintessential example."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-22T10:22:14-05:00"><meta property="article:modified_time" content="2025-11-22T10:22:14-05:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="We didn't get the AI failure modes that philosophy anticipated"><meta name=twitter:description content="The original idea of AI, that we got mostly through science-fiction, and also a
little from the philosophy of mind and logic, imagined an entity that would
implement idealized and mechanical notions of thoughts, reasoning and logic.
Such an entity would of course know everything there is to know about such
topics, and its behavior would thus be rooted in them. Although this would mean
that the entity would generally behave in impressive and powerful ways, it was
also implicitly understood that sometimes this &ldquo;perfection&rdquo; would lead to
paradoxical behaviors and &ldquo;errors&rdquo;: the robot stuck in a circle in the Asimov
story is the quintessential example."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://cjauvin.github.io/posts/"},{"@type":"ListItem","position":2,"name":"We didn't get the AI failure modes that philosophy anticipated","item":"https://cjauvin.github.io/posts/ai-failure-mode/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"We didn't get the AI failure modes that philosophy anticipated","name":"We didn\u0027t get the AI failure modes that philosophy anticipated","description":"The original idea of AI, that we got mostly through science-fiction, and also a little from the philosophy of mind and logic, imagined an entity that would implement idealized and mechanical notions of thoughts, reasoning and logic. Such an entity would of course know everything there is to know about such topics, and its behavior would thus be rooted in them. Although this would mean that the entity would generally behave in impressive and powerful ways, it was also implicitly understood that sometimes this \u0026ldquo;perfection\u0026rdquo; would lead to paradoxical behaviors and \u0026ldquo;errors\u0026rdquo;: the robot stuck in a circle in the Asimov story is the quintessential example.\n","keywords":[],"articleBody":"The original idea of AI, that we got mostly through science-fiction, and also a little from the philosophy of mind and logic, imagined an entity that would implement idealized and mechanical notions of thoughts, reasoning and logic. Such an entity would of course know everything there is to know about such topics, and its behavior would thus be rooted in them. Although this would mean that the entity would generally behave in impressive and powerful ways, it was also implicitly understood that sometimes this “perfection” would lead to paradoxical behaviors and “errors”: the robot stuck in a circle in the Asimov story is the quintessential example.\nSomeone could argue that HAL 9000, in the 2001 movie, is having deeper mental issues, which seems on first reading to be beyond the realm of logic and pure thought. But the schizophrenia of HAL stems from a deep cognitive dissonance and contradiction between his mission and the way he needs to deal with the human crew. So at the heart, we understand that this is ultimately logical and philosophical in nature, and it goes with the classical AI vision of its time.\nNow contrast that with what we got in the present days, ChatGPT and GenAI, which is working, and failing, in ways that were very hard to foresee. When ChatGPT malfunctions, it’s rarely because of a logical paradox. Rather, most of the times, the situation will go something like this:\nYou: format this article with the citations in the APA style ChatGPT: You: I wanted the APA style! ChatGPT: You: if you knew all along, why did you give it wrong the first time? ChatGPT: … This type of error is weird, and takes time to get used to, because it’s not at all what we had in mind, when we were thinking about how computers make mistakes. This is a type of mistake that is more akin to someone contradicting himself, and, when confronted to it, just says: oh, sorry about that, while merrily going to his next contradiction.\n","wordCount":"356","inLanguage":"en","datePublished":"2025-11-22T10:22:14-05:00","dateModified":"2025-11-22T10:22:14-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://cjauvin.github.io/posts/ai-failure-mode/"},"publisher":{"@type":"Organization","name":"Christian Jauvin","logo":{"@type":"ImageObject","url":"https://cjauvin.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://cjauvin.github.io/ accesskey=h title="Christian Jauvin (Alt + H)">Christian Jauvin</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">We didn't get the AI failure modes that philosophy anticipated</h1><div class=post-meta><span title='2025-11-22 10:22:14 -0500 -0500'>November 22, 2025</span></div></header><div class=post-content><p>The original idea of AI, that we got mostly through science-fiction, and also a
little from the philosophy of mind and logic, imagined an entity that would
implement idealized and mechanical notions of thoughts, reasoning and logic.
Such an entity would of course know everything there is to know about such
topics, and its behavior would thus be rooted in them. Although this would mean
that the entity would generally behave in impressive and powerful ways, it was
also implicitly understood that sometimes this &ldquo;perfection&rdquo; would lead to
paradoxical behaviors and &ldquo;errors&rdquo;: the robot stuck in a circle in the Asimov
story is the quintessential example.</p><p><img loading=lazy src=/images/asimov.png></p><p>Someone could argue that HAL 9000, in the 2001 movie, is having deeper mental
issues, which seems on first reading to be beyond the realm of logic and pure
thought. But the schizophrenia of HAL stems from a deep cognitive dissonance and
contradiction between his mission and the way he needs to deal with the human
crew. So at the heart, we understand that this is ultimately logical and
philosophical in nature, and it goes with the classical AI vision of its time.</p><p><img loading=lazy src=/images/hal9k.png></p><p>Now contrast that with what we got in the present days, ChatGPT and GenAI, which
is working, and failing, in ways that were very hard to foresee. When ChatGPT
malfunctions, it&rsquo;s rarely because of a logical paradox. Rather, most of the
times, the situation will go something like this:</p><ul><li>You: format this article with the citations in the APA style</li><li>ChatGPT: &lt;gives it to you, with the wrong style></li><li>You: I wanted the APA style!</li><li>ChatGPT: &lt;gives it to you, with the right style this time></li><li>You: if you knew all along, why did you give it wrong the first time?</li><li>ChatGPT: &mldr;</li></ul><p>This type of error is weird, and takes time to get used to, because it&rsquo;s not at
all what we had in mind, when we were thinking about how computers make
mistakes. This is a type of mistake that is more akin to someone contradicting
himself, and, when confronted to it, just says: oh, sorry about that, while
merrily going to his next contradiction.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://cjauvin.github.io/>Christian Jauvin</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>