<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Does ChatGPT know what is a question? | Christian Jauvin</title><meta name=keywords content><meta name=description content="I was explaining to a friend recently that ChatGPT, to its core, is &ldquo;just&rdquo; a
model to predict the next word, the one coming after a bunch of other words.
So when you ask it &ldquo;What is the capital of France?&rdquo;, it does not (really) answer
your question, it completes a sequence of words on which it has been trained,
deeply and efficiently.
So considering that, it might seem that ChatGPT is in a situation that would be
akin to you, if someone tells you a bunch of words you don&rsquo;t understand (in a
foreign language say) and then, someone else gives you a card, on which you can
find some words to pronounce, as a reply (in a language that you don&rsquo;t
understand but can read, let&rsquo;s say)."><meta name=author content="Christian Jauvin"><link rel=canonical href=https://cjauvin.github.io/posts/llm-question-answering/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://cjauvin.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://cjauvin.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://cjauvin.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://cjauvin.github.io/apple-touch-icon.png><link rel=mask-icon href=https://cjauvin.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://cjauvin.github.io/posts/llm-question-answering/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><style>.first-entry.home-info{min-height:auto!important;margin:calc(var(--gap)/2)0 var(--gap)!important}.first-entry.home-info .entry-content{margin:8px 0!important}.first-entry.home-info .entry-footer{margin-top:8px!important}.social-icons a{padding:5px!important}.main{padding:calc(var(--gap)/2)var(--gap)!important}.center{text-align:center}.center img{display:block;margin-left:auto;margin-right:auto}</style><meta property="og:url" content="https://cjauvin.github.io/posts/llm-question-answering/"><meta property="og:site_name" content="Christian Jauvin"><meta property="og:title" content="Does ChatGPT know what is a question?"><meta property="og:description" content="I was explaining to a friend recently that ChatGPT, to its core, is “just” a model to predict the next word, the one coming after a bunch of other words.
So when you ask it “What is the capital of France?”, it does not (really) answer your question, it completes a sequence of words on which it has been trained, deeply and efficiently.
So considering that, it might seem that ChatGPT is in a situation that would be akin to you, if someone tells you a bunch of words you don’t understand (in a foreign language say) and then, someone else gives you a card, on which you can find some words to pronounce, as a reply (in a language that you don’t understand but can read, let’s say)."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-24T11:21:12-05:00"><meta property="article:modified_time" content="2026-02-24T11:21:12-05:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Does ChatGPT know what is a question?"><meta name=twitter:description content="I was explaining to a friend recently that ChatGPT, to its core, is &ldquo;just&rdquo; a
model to predict the next word, the one coming after a bunch of other words.
So when you ask it &ldquo;What is the capital of France?&rdquo;, it does not (really) answer
your question, it completes a sequence of words on which it has been trained,
deeply and efficiently.
So considering that, it might seem that ChatGPT is in a situation that would be
akin to you, if someone tells you a bunch of words you don&rsquo;t understand (in a
foreign language say) and then, someone else gives you a card, on which you can
find some words to pronounce, as a reply (in a language that you don&rsquo;t
understand but can read, let&rsquo;s say)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://cjauvin.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Does ChatGPT know what is a question?","item":"https://cjauvin.github.io/posts/llm-question-answering/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Does ChatGPT know what is a question?","name":"Does ChatGPT know what is a question?","description":"I was explaining to a friend recently that ChatGPT, to its core, is \u0026ldquo;just\u0026rdquo; a model to predict the next word, the one coming after a bunch of other words.\nSo when you ask it \u0026ldquo;What is the capital of France?\u0026rdquo;, it does not (really) answer your question, it completes a sequence of words on which it has been trained, deeply and efficiently.\nSo considering that, it might seem that ChatGPT is in a situation that would be akin to you, if someone tells you a bunch of words you don\u0026rsquo;t understand (in a foreign language say) and then, someone else gives you a card, on which you can find some words to pronounce, as a reply (in a language that you don\u0026rsquo;t understand but can read, let\u0026rsquo;s say).\n","keywords":[],"articleBody":"I was explaining to a friend recently that ChatGPT, to its core, is “just” a model to predict the next word, the one coming after a bunch of other words.\nSo when you ask it “What is the capital of France?”, it does not (really) answer your question, it completes a sequence of words on which it has been trained, deeply and efficiently.\nSo considering that, it might seem that ChatGPT is in a situation that would be akin to you, if someone tells you a bunch of words you don’t understand (in a foreign language say) and then, someone else gives you a card, on which you can find some words to pronounce, as a reply (in a language that you don’t understand but can read, let’s say).\nSo in a sense, you, and ChatGPT, are in situation that can be compared to the operator inside Searle’s Chinese Room: you can efficiently and procedurally manipulate a bunch of symbols, but they are meaningless to you. You are blind to their real meaning.\nBut then what is, in essence, the act of answering a question? Why would ChatGPT’s way of answering feel “less” authentic than our way, exactly? What does it even mean, to understand that something is a question? Wittgenstein would probably say that all the situations involving answering are part of a broad “language game” of human behaviors that we call, generally, “answering a question”. There is no deep and unique essence of “answer-ness”, or “question-asking-ness” : there are a multitude of loosely related behaviors, involving context and language, that we call such.\nSo I think that the problem that this poses is really a problem of a priori modeling : for as long as the idea of AI existed, the a priori model of language understanding (and by implication question answering) has always been : you receive a sequence of symbols (words), and then you have some kind of processing mechanism, that is in charge of “understanding” these words, that is, build some kind of (internal) representation of their meaning, whatever that “thing” might be (some internal mental model, state, or configuration, whatever). Only from that point, once you have that, can something follow (an answer, in the case of question answering), that could be first conceived, and then executed.\nBut then it seems that large language modeling puts that model into question. It seems that real understanding can emerge from the mere fact of adequately learning to predict a sequence, without the need for that “intermediate”, black box modeling, and that is a very deep and unsettling change of paradigm.\n","wordCount":"434","inLanguage":"en","datePublished":"2026-02-24T11:21:12-05:00","dateModified":"2026-02-24T11:21:12-05:00","author":{"@type":"Person","name":"Christian Jauvin"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://cjauvin.github.io/posts/llm-question-answering/"},"publisher":{"@type":"Organization","name":"Christian Jauvin","logo":{"@type":"ImageObject","url":"https://cjauvin.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://cjauvin.github.io/ accesskey=h title="Christian Jauvin (Alt + H)">Christian Jauvin</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Does ChatGPT know what is a question?</h1><div class=post-meta><span title='2026-02-24 11:21:12 -0500 -0500'>February 24, 2026</span>&nbsp;·&nbsp;Christian Jauvin</div></header><div class=post-content><p>I was explaining to a friend recently that ChatGPT, to its core, is &ldquo;just&rdquo; a
model to predict the next word, the one coming after a bunch of other words.</p><p>So when you ask it &ldquo;What is the capital of France?&rdquo;, it does not (really) answer
your question, it completes a sequence of words on which it has been trained,
deeply and efficiently.</p><p>So considering that, it might seem that ChatGPT is in a situation that would be
akin to you, if someone tells you a bunch of words you don&rsquo;t understand (in a
foreign language say) and then, someone else gives you a card, on which you can
find some words to pronounce, as a reply (in a language that you don&rsquo;t
understand but can read, let&rsquo;s say).</p><p class=center><img loading=lazy src=/posts/llm-question-answering/llm-question-searle.png></p><p>So in a sense, you, and ChatGPT, are in situation that can be compared to the
operator inside Searle&rsquo;s Chinese Room: you can efficiently and procedurally
manipulate a bunch of symbols, but they are meaningless to you. You are blind to
their real meaning.</p><p>But then what is, in essence, the act of answering a question? Why would
ChatGPT&rsquo;s way of answering feel &ldquo;less&rdquo; authentic than our way, exactly? What
does it even mean, to understand that <em>something</em> is a question? Wittgenstein
would probably say that all the situations involving answering are part of a
broad &ldquo;language game&rdquo; of human behaviors that we call, generally, &ldquo;answering a
question&rdquo;. There is no deep and unique essence of &ldquo;answer-ness&rdquo;, or
&ldquo;question-asking-ness&rdquo; : there are a multitude of loosely related behaviors,
involving context and language, that we call such.</p><p>So I think that the problem that this poses is really a problem of a priori
modeling : for as long as the idea of AI existed, the a priori model of language
understanding (and by implication question answering) has always been : you
receive a sequence of symbols (words), and then you have some kind of processing
mechanism, that is in charge of &ldquo;understanding&rdquo; these words, that is, build some
kind of (internal) representation of their meaning, whatever that &ldquo;thing&rdquo; might
be (some internal mental model, state, or configuration, whatever). Only from
that point, once you have that, can something follow (an answer, in the case of
question answering), that could be first conceived, and then executed.</p><p class=center><img loading=lazy src=/posts/llm-question-answering/llm-question-answer-classic-ai.png></p><p>But then it seems that large language modeling puts that model into question. It
seems that real understanding can emerge from the mere fact of adequately
learning to predict a sequence, without the need for that &ldquo;intermediate&rdquo;, black
box modeling, and that is a very deep and unsettling change of paradigm.</p><p class=center><img loading=lazy src=/posts/llm-question-answering/llm-question-answer-wittgenstein.png></p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://cjauvin.github.io/>Christian Jauvin</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>